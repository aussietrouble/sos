%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Some commands and examples in this template have been taken 
% from the template for CS 271 by Alistair Sinclair at UC Berkeley
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt]{article}

%% File containing various commands and macros
\input{macros}


%%% Define any additional macros here
\newcommand{\ds}{\displaystyle}
\usepackage{tikz}

%%% Optimization problem (Use as a template if there are multiple constraints)
%%% Usage: \optprob{<minimize or maximize>}{<variables>}{<objective>}{<constraint>}{<reference label>}
\newcommand{\optprob}[5]
{
	\begin{equation}\label{#5}
	\begin{aligned}
	& \underset{#2}{\text{#1}}
	& & #3 \\
	& \text{\quad\ s.t.}
	& & #4
	\end{aligned}
	\end{equation}
}

\begin{document}
%% \titlebox{<top-left>}{<top-right>}{<title>}{<bottom-left>}{<bottom-right>}
\titlebox{Research Notes}{}{Nonparametric Estimation using SOS-Convexity}{Prof. John Lafferty}{Students: YJ Choe, Max Cytrynbaum, Wei Hu}

\section{Introduction}

(YJ will write out this section summerizing our first-day discussion when he has time.)

\clearpage

\section{SOS-Convex Regression}

Given $\{(\bfx_i, y_i)\}_{i=1}^n$, where $\bfx_i \in \R^p$ and $y_i \in \R$ for $i = 1, \dotsc, n$, recall that we have the equivalence between the following optimization problems:

\optprob{minimize}{}{\sum_{i=1}^n\inparen{y_i - f(\bfx_i)}^2}{\text{$f$ is convex.}}{cvx-reg}

\optprob{minimize}{\bfz, \bfbeta}{\sum_{i=1}^n\inparen{y_i - z_i}^2}{z_j \geq z_i + \bfbeta_i^T(\bfx_j - \bfx_i) \qquad\forall\ i, j = 1, \dotsc, n.}{cvx-reg-qp}

In particular, we can reduce the infinite-dimensional problem \eqref{cvx-reg} into a finite-dimensional quadratic program (QP) \eqref{cvx-reg-qp}, which can be efficiently solved. The solution to \eqref{cvx-reg-qp} can be viewed as a piecewise-linear convex function. 

Here, we attempt to derive the analogous equivalence, i.e. find an equivalent convex optimization problem to the following optimization problem:
\optprob{minimize}{}{\sum_{i=1}^n\inparen{y_i - f(\bfx_i)}^2}{\text{$f$ is an SOS-convex polynomial of degree $2d$.}}{sos-reg}

Denote the vector of basis monomials up to degree $k$ by $\bfv_k(\bfx) = (1, x_1, \dotsc, x_p, x_1^2, x_1x_2, \dotsc, x_p^k)^T$, where $\bfx = (x_1, \dotsc, x_p)$. Then the length of $\bfv_k(\bfx)$ is ${k + p \choose p}$. Let
$$A_k = \left\{ \bfalpha = (\alpha_1, \dotsc, \alpha_p) \in \N^p \biggl| \sum_{j=1}^p \alpha_j \leq k \right\}.$$

Then, we may represent $f$ by a coefficient vector $\bftheta \in \R^s$($s = {2d + p \choose p}$), such that
\[
f(\bfx) = \bftheta^T\bfv_{2d}(\bfx) = \sum_{\bfalpha \in A_{2d}} \theta_\bfalpha \bfx^\bfalpha,
\]
where $\bfx^\bfalpha = x_1^{\alpha_1}\cdots x_p^{\alpha_p}$. Note the one-to-one correspondence between $f$ and $\bftheta$.

Further, as done with the convex program, we introduce the auxiliary variable $\bfz = (z_1, \dotsc, z_n)$ so that 
\begin{equation}\label{aux}
f(\bfx_i) = \bftheta^T\bfv_{2d}(\bfx_i) = z_i \qquad\forall\ i = 1, \dotsc, n.
\end{equation}
We can write this more concisely by introducing the matrix
\[
V = V(\bfx_1, \dotsc, \bfx_n) = \begin{bmatrix}
\bfv_{2d}(\bfx_1)^T \\
\vdots \\
\bfv_{2d}(\bfx_n)^T
\end{bmatrix}_{n \times s}
\]
so that \eqref{aux} simply becomes
\begin{equation}\label{sos-poly}
V\bftheta = \bfz.
\end{equation}

So we have a linear constraint on the coefficient $\bftheta$ that is equivalent to saying that the polynomial interpolates the points $\{(\bfx_i, z_i)\}_{i=1}^n$. Analogously, we can rewrite the objective to be
\begin{equation}\label{sos-reg-obj2}
\sum_{i=1}^n(y_i - z_i)^2 = \norm{\bfy - \bfz}^2
\end{equation}
where $\bfy = (y_1, \dotsc, y_n)$ and $\bfz = (z_1, \dotsc, z_n)$.


Now we want to rewrite the constraint that $f(\bfx) = \bftheta^T\bfv_{2d}(\bfx)$ is SOS-convex. Recall that $f$ is SOS-convex if and only if the polynomial $\bfu^TH_\bftheta(\bfx)\bfu$ is sos in $(\bfx, \bfu) \in \mathbb R^{2p}$, where $H_\bftheta(\bfx)$ is the Hessian of $f$.

For $i, j\in\{1, \cdots, p\}$ we have
\begin{equation} \label{hessian}
H_\bftheta(\bfx)_{ij} = \frac{\partial^2 f(\bfx)}{\partial x_i \partial x_j}  = \sum_{\bfalpha \in A_{2d} } \theta_\bfalpha \frac{\partial^2 \bfx^\bfalpha}{\partial x_i \partial x_j} = 
\begin{cases}
\sum\limits_{\bfalpha \in A_{2d} } \theta_\bfalpha \alpha_i \alpha_j \bfx^{\bfbeta_{\bfalpha, i,j}} & (i \not= j)\\
\sum\limits_{\bfalpha \in A_{2d}} \theta_\bfalpha \alpha_i (\alpha_i-1) \bfx^{\bfbeta_{\bfalpha, i,i}} & (i = j)
\end{cases}
= \sum_{\bfalpha \in A_{2d} } c_{\bfalpha, i, j} \theta_\bfalpha  \bfx^{\bfbeta_{\bfalpha, i,j}}
\end{equation}
where
\begin{equation}\notag
\bfbeta_{\bfalpha, i,j} =
\begin{cases}
 (\alpha_1, \dotsc, \max(\alpha_i-1, 0), \dotsc, \max(\alpha_j-1, 0), \dotsc, \alpha_p) & i \neq j\\
(\alpha_1, \dotsc, \max(\alpha_i-2, 0), \dotsc, \alpha_p) & i = j
\end{cases}
\end{equation}
and
\begin{equation}\notag
c_{\bfalpha, i, j} =
\begin{cases}
\alpha_i \alpha_j  & i \not= j\\
\alpha_i (\alpha_i - 1)  & i = j.
\end{cases}
\end{equation}
%Note that both $\bfbeta_{\bfalpha, i,j}$ and $c_{\bfalpha, i, j}$ are functions of $\bfalpha, i, j$ and can be calculated.

Then we have
\begin{equation}\notag
\bfu^TH_\bftheta(\bfx)\bfu = \sum_{i,j=1}^p \inparen{ \sum_{\bfalpha \in A_{2d}} c_{\bfalpha, i, j} \theta_\bfalpha  \bfx^{\bfbeta_{\bfalpha, i,j}} } u_iu_j,
\end{equation}
which can be further written as
\begin{equation} \label{lhs}
\bfu^TH_\bftheta(\bfx)\bfu = \sum_{1 \le i \le j \le p}  \sum_{\bfbeta \in A_{2d - 2}} h_\bftheta (\bfbeta, i, j) \bfx^{\bfbeta}  u_iu_j
\end{equation}
where
\begin{equation}\notag
h_\bftheta (\bfbeta, i, j) =
\begin{cases}
(\beta_i + 2) (\beta_i + 1) \theta_{(\beta_1, \cdots, \beta_i + 2, \cdots, \beta_p)} & i = j\\
2(\beta_i + 1) (\beta_j + 1) \theta_{(\beta_1, \cdots, \beta_i + 1, \cdots, \beta_j + 1 \cdots, \beta_p)} & i < j.
\end{cases}
\end{equation}

It is then easy to see that $\bfu^TH_\bftheta(\bfx)\bfu$ is SOS if and only if
\begin{equation} \label{sos-cvx}
\bfu^TH_\bftheta(\bfx)\bfu = \bfv_d'(\bfx, \bfu)^T Q \bfv_d'(\bfx, \bfu)
\end{equation}
\begin{equation} \label{sos-cvx-psd}
Q \succeq 0
\end{equation}
where $\bfv_d'(\bfx, \bfu)$ is the vector of all monomials in $(\bfx, \bfu)$ in which the degrees of all $x_i$'s have sum at most $d-1$ and there is exactly one $u_i$, i.e., 
$$\bfv_d'(\bfx, \bfu) = (u_1 \bfv_{d-1}(\bfx)^T, u_2 \bfv_{d-1}(\bfx)^T, \cdots, u_p \bfv_{d-1}(\bfx)^T )^T.$$
The length of $\bfv_d'(\bfx, \bfu)$ is $r = p{p+d-1 \choose p}$. $Q$ is a $r \times r$ matrix.

\eqref{sos-cvx} is not a valid semidefinite constraint yet, because it is an equality between two polynomials. This means we want to equate the \emph{coefficients} of the two polynomials on $(\bfx, \bfu)$. The left-hand side is given by \eqref{lhs}. Further, we can express the right-hand side in terms of their coordinates in the following way. First define the coordinate matrix $B_{\bfbeta, i, j}$ for each $\bfbeta \in A_{2d - 2}, 1 \le i \le j \le p$ such that
\[
\bfv_d'(\bfx, \bfu) \bfv_d'(\bfx, \bfu)^T = \sum_{1 \le i \le j \le p}  \sum_{\bfbeta \in A_{2d - 2}} B_{\bfbeta, i, j} \bfx^{\bfbeta}  u_iu_j.
\]
Note that the matrices $B_{\bfbeta, i, j}$'s are simply ``constants'', i.e. they only depend on $d$ (and $p$). With this, the right-hand side of \eqref{sos-cvx} becomes
\begin{align}\label{rhs}
\bfv_d'(\bfx,\bfu)^T Q \bfv_d'(\bfx,\bfu) &= \tr(Q \bfv_d'(\bfx, \bfu) \bfv_d'(\bfx, \bfu)^T) \notag\\
&= \ip{Q, \bfv_d'(\bfx, \bfu) \bfv_d'(\bfx, \bfu)^T} \notag\\
&= \ip{Q, \sum_{1 \le i \le j \le p} \sum_{\bfbeta \in A_{2d - 2}} B_{\bfbeta, i, j} \bfx^{\bfbeta}  u_iu_j} \notag\\
&= \sum_{1 \le i \le j \le p} \sum_{\bfbeta \in A_{2d - 2}} \ip{Q, B_{\bfbeta, i, j}} \bfx^{\bfbeta}  u_iu_j
\end{align}
where $\ip{A,B} = \tr(A^TB) = \sum_{i,j} A_{ij}B_{ij}$ is the  matrix inner product. Note that $Q$ is symmetric.

Then, we can equate the coefficients of \eqref{lhs} and \eqref{rhs} to obtain:
\begin{align}
\ip{Q, B_{\bfbeta, i, j}} = h_\bftheta(\bfbeta, i, j) \qquad \forall\ \bfbeta \in A_{2d - 2}, 1 \le i \le j \le p \label{sos-cvx-coeff}
%\ip{Q, B_{\bfgamma}} &= 0 \qquad\qquad \text{for all other $\bfgamma$} \label{sos-cvx-coeff2}
\end{align}

Putting \eqref{sos-poly}, \eqref{sos-reg-obj2}, \eqref{sos-cvx-psd}, and \eqref{sos-cvx-coeff} together, \eqref{sos-reg} can be restated as the following problem:

\begin{equation}\label{sos-reg2}
\begin{aligned}
& \underset{\bfz, \bftheta, Q}{\text{minimize}}
& & \norm{\bfy-\bfz}^2 \\
& \text{\quad\ s.t.}
& &  V\bftheta = \bfz \\
& & & \langle{Q, B_{\bfbeta, i, j}} \rangle = h_\bftheta(\bfbeta, i, j) \qquad \forall\ \bfbeta \in A_{2d - 2}, 1 \le i \le j \le p \\
%& & & \langle{Q, B_{\bfgamma}\rangle} = 0 \qquad \text{for all other $\bfgamma$} \\
& & & Q \psd 0 
\end{aligned}
\end{equation}

\eqref{sos-reg2} is almost an SDP, except that the objective is quadratic. But in general, we can introduce another auxiliary variable $t$ to restate the problem as
\begin{equation}\label{sos-reg3}
\begin{aligned}
& \underset{t, \bfz, \bftheta, Q}{\text{minimize}}
& & t \\
& \text{\quad\ s.t.}
& & \norm{\bfy-\bfz}^2 \leq t \\
& & & V\bftheta = \bfz \\
& & & \langle{Q, B_{\bfbeta, i, j}} \rangle = h_\bftheta(\bfbeta, i, j) \qquad \forall\ \bfbeta \in A_{2d - 2}, 1 \le i \le j \le p \\
%& & & \langle{Q, B_{\bfgamma}\rangle} = 0 \qquad \text{for all other $\bfgamma$} \\
& & & Q \psd 0 
\end{aligned}
\end{equation}

Then, we are left with a quadratic inequality constraint. Fortunately, the following allows us to convert this into a semidefinite constraint.
\begin{lemma}
For any $\bfx, \bfq \in \R^p$ and $r \in \R$, $\bfx^T\bfx + \bfq^T\bfx + r \leq 0$ if and only if $\begin{bmatrix} I & -\bfx \\ -\bfx^T & -\bfq^T\bfx - r \end{bmatrix} \psd 0$.
\end{lemma}
\begin{proof}
For any $\bfy \in \R^p$ and $z \in \R$, 
\begin{align*}
\begin{bmatrix} \bfy^T & z \end{bmatrix} \begin{bmatrix} I & -\bfx \\ -\bfx^T & -\bfq^T\bfx - r \end{bmatrix} \begin{bmatrix} \bfy \\ z \end{bmatrix} &= \bfy^T\bfy -2z\bfx^T\bfy - z^2(\bfq^T\bfx+r) \\
&= \norm{\bfy - z\bfx}^2 - z^2(\bfx^T\bfx + \bfq^T\bfx + r).
\end{align*}
If $\bfx^T\bfx + \bfq^T\bfx + r \leq 0$, then this is nonnegative for all $\bfy \in \R^p$ and $z \in \R$. Otherwise, one can find $\bfy \in \R^p$ and $z \in \R$ such that this is strictly negative.
\end{proof}

Thus,
\begin{align*}
\norm{\bfy-\bfz}^2 \leq t &\iff \bfz^T\bfz - 2\bfy^T\bfz + (\bfy^T\bfy - t) \leq 0 \\
&\iff \begin{bmatrix} I & -\bfz \\ -\bfz^T & 2\bfy^T\bfz - \bfy^T\bfy + t \end{bmatrix} \psd 0.
\end{align*}
Note that the last relation is a linear matrix inequality (LMI), i.e. it says that a linear combination of symmetric matrices is positive semidefinite. 

Thus, we can now write \eqref{sos-reg3} into a semidefinite program:
\begin{equation}\label{sos-reg4}
\begin{aligned}
& \underset{t, \bfz, \bftheta, Q}{\text{minimize}}
& & t \\
& \text{\quad\ s.t.}
& & \begin{bmatrix} I & -\bfz \\ -\bfz^T & 2\bfy^T\bfz - \bfy^T\bfy + t \end{bmatrix} \psd 0 \\
& & & V\bftheta = \bfz \\
& & & \langle{Q, B_{\bfbeta, i, j}} \rangle = h_\bftheta(\bfbeta, i, j) \qquad \forall\ \bfbeta \in A_{2d - 2}, 1 \le i \le j \le p \\
%& & & \langle{Q, B_{\bfgamma}\rangle} = 0 \qquad \text{for all other $\bfgamma$} \\
& & & Q \psd 0 
\end{aligned}
\end{equation}
where the two semidefinite constraints can be restated -- if necessary -- into one semidefinite constraint
\[
\begin{bmatrix} I & -\bfz &  \\ -\bfz^T & 2\bfy^T\bfz - \bfy^T\bfy + t & \\ & & Q \end{bmatrix} \psd 0.
\]

Finally, note that the entire program depends on the degree of the SOS-convex polynomial that we started off with: $2d$. 

\subsection*{Further Questions}

\begin{enumerate}
\item What is the program size? Is it tractable?
%\item Can the zero constraints be simplified?
\item For any given $d$, is the program feasible? What is the behavior of the objective $t_d$?
\item How can SDP hierarchy (e.g. by Lasserre) help choosing/removing $d$?
\end{enumerate}

\clearpage
\section{Convexity Pattern Problem}

We now consider a more restricted family of distributions that are hopefully more tractable and also have interesting applications.

With the familiar regression setting as in \eqref{cvx-reg}, first consider the additional constraint that $f$ is not only convex but also a function of only a few variables from $\bfx = (x_1, \dotsc, x_p)$. For example, we may have
\[
f(x_1, \dotsc, x_p) = f(x_1, x_2) \qquad \forall\ \bfx \in \R^p
\]
as one of the possibilities. 

In \cite{DCM}, Qi, Xu, and Lafferty shows a way to approximate the solution to the above problem \emph{additively}. Specifically, this is 
\optprob{minimize}{f_1, \dotsc, f_p}{\sum_{i=1}^n{(y_i - \sum_{j=1}^p f_j(x_{ij}))^2}}{f_1, \dotsc, f_p \text{ convex}}{cvx-additive-reg}
where $\bfx_i = (x_{i1}, \dotsc, x_{ip}) \in \R^p$. In other words, we have the model
\[
Y = \sum_{j=1}^p f_j(X_j) + \e
\]
in the population, with random variables $X = (X_1, \dotsc, X_p) \in \R^p$ and $Y \in \R$.

We can view this as a problem of \emph{sparsity patterns}, i.e. whether each variable is ``relevant'' ($f_j \not\equiv 0$) or not ($f_j \equiv 0$), and it is clear that there are $2^p$ sparsity patterns with $p$ variables.

\textbf{Here, we consider an analogous problem of choosing whether each $f_j$ is convex or concave.} Naturally, there are $2^p$ \emph{convexity patterns}. We can write this problem as the following optimization problem:
\begin{equation}\label{cvx-pattern}
\begin{aligned}
& \underset{\bfZ, \bff, \bfg}{\text{minimize}}
& & \sum_{i=1}^n \inparen{y_i - \sum_{j=1}^p\insquare{Z_jf_j(x_{ij}) + (1-Z_j)g_j(x_{ij})}}^2 \\
& \text{\quad\ s.t.}
& & Z_1, \dotsc, Z_p \in \{0,1\} \\
& & & f_1, \dotsc, f_p \text{ convex} \\
& & & g_1, \dotsc, g_p \text{ concave} 
\end{aligned}
\end{equation}
Note that $Z_1, \dotsc, Z_p$ are 0/1-boolean variables and $f_1, \dotsc, f_p, g_1, \dotsc, g_p$ are univariate functions.

In order to make the problem more tractable, we first give extra constraints: namely, that $f_1, \dotsc, f_p, g_1, \dotsc, g_p$ are \emph{polynomials}. It is important to note that a univariate polynomial is convex if and only if it is SOS-convex. [Problem: Is the set of convex polynomials dense in the set of convex functions? Is this relevant?] We can rewrite the program as follows:
\begin{equation}\label{cvx-pattern2}
\begin{aligned}
& \underset{\bfZ, \bff, \bfg}{\text{minimize}}
& & \sum_{i=1}^n \inparen{y_i - \sum_{j=1}^p\insquare{Z_jf_j(x_{ij}) + (1-Z_j)g_j(x_{ij})}}^2 \\
& \text{\quad\ s.t.}
& & Z_1, \dotsc, Z_p \in \{0,1\} \\
& & & f_1, \dotsc, f_p \text{ are (SOS-)convex polynomials of degree at most $d$} \\
& & & g_1, \dotsc, g_p \text{ are (SOS-)concave polynomials of degree at most $d$} 
\end{aligned}
\end{equation}
Using the similar trick as above, we hope to convert the constraints on $f_1, \dotsc, f_p, g_1, \dotsc, g_p$ into linear or semidefinite ones. 

A more important feature of this program is the use of 0-1 variables. It is well-known that, in general, solving a 0-1 integer linear program is NP-hard, and one of the standard procedures in theoretical computer science in dealing with this problem is to relax it such that the boolean constraint is replaced by $Z_1, \dotsc, Z_p \in [0,1]$, or equivalently the quadratic constraint $Z_j^2 - Z_j \leq 0\ \forall j=1, \dotsc, p$.

With this relaxation comes a family of LP/SDP hierarchies, such as the ones developed by Lov\'asz-Schrijver, Sherali-Adams, and Lasserre. [Prof. Madhur Tulsiani's Survey] These hierarchies are all a sequence of convex programs (LPs or SDPs) whose objective approaches the actual 0-1 solution. 

A good way to think about the hierarchies for 0-1 programs is to consider the $Z_j$'s the marginals of a distribution over a set of 0-1 solutions. Specifically, in the initial ``round'', consider $Z_j$ to be the marginal of the solution whose $j$th entry is 1 and all others are zero. Then, in consecutive rounds, the goal is to add the \emph{joint probabilities} between these variables -- in the $r$th round, we consider the joint random variables $Z_S$ for each $S \subseteq \{1, \dotsc, p\}$ such that $\abs{S} \leq r$. One can think of these ``big variables'' as $Z_S = \ex{\prod_{j \in S} Z_j}$, i.e. the probability that all variables in $S$ are 1.

Our hope is to use one of the hierarchies to solve a set of relaxations of \eqref{cvx-pattern2} that approximates the actual solution efficiently.

\clearpage
\section{Log-SOS-Concave Density Estimation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% I have actually re-defined the \cite{} command so that the
% citation will simply display as whatever you name it to be.
%
% There is no bibtex here that you need to run. If you say
% \cite{foo}, the lecture notes will simply display [foo].
%
% You don't need a .bib file. You will need to manually put in
% entries in the format below for whatever references you cite.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section*{References}
\beginrefs
\bibentry{BV}{\sc Boyd, S.} and {\sc Vandenberghe, L.} (2009).
{\it Convex Optimization}.
Cambridge University Press.
\bibentry{Lasserre}{\sc Lasserre, J. B.} (2009).
{\it Moments, Positive Polynomials and Their Applications}. Vol. 1. 
World Scientific.
\bibentry{DCM}{\sc Qi, Y.}, {\sc Xu, M.}, and {\sc Lafferty, J.} (2014).
{\it Learning High-Dimensional Concave Utility Functions for Discrete Choice Models}.
NIPS Submission.
\endrefs



% **** THIS ENDS THE EXAMPLES. DON'T DELETE THE FOLLOWING LINE:

\end{document}
