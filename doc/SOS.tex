%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Some commands and examples in this template have been taken 
% from the template for CS 271 by Alistair Sinclair at UC Berkeley
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt]{article}

%% File containing various commands and macros
\input{macros}


%%% Define any additional macros here
\newcommand{\ds}{\displaystyle}
\usepackage{tikz}

%%% Optimization problem (Use as a template if there are multiple constraints)
%%% Usage: \optprob{<minimize or maximize>}{<variables>}{<objective>}{<constraint>}{<reference label>}
\newcommand{\optprob}[5]
{
	\begin{equation}\label{#5}
	\begin{aligned}
	& \underset{#2}{\text{#1}}
	& & #3 \\
	& \text{\quad\ s.t.}
	& & #4
	\end{aligned}
	\end{equation}
}

\begin{document}
%% \titlebox{<top-left>}{<top-right>}{<title>}{<bottom-left>}{<bottom-right>}
\titlebox{Research Notes}{}{Nonparametric Estimation using SOS-Convexity}{Prof. John Lafferty}{Students: YJ Choe, Max Cytrynbaum, Wei Hu}

\section{Introduction}

(YJ will write out this section summerizing our first-day discussion when he has time.)

\clearpage

\section{SOS-Convex Regression}

Given $\{(\bfx_i, y_i)\}_{i=1}^n$, where $\bfx_i \in \R^p$ and $y_i \in \R$ for $i = 1, \dotsc, n$, recall that we have the equivalence between the following optimization problems:

\optprob{minimize}{}{\sum_{i=1}^n\inparen{y_i - f(\bfx_i)}^2}{\text{$f$ is convex.}}{cvx-reg}

\optprob{minimize}{\bfz, \bfbeta}{\sum_{i=1}^n\inparen{y_i - z_i}^2}{z_j \geq z_i + \bfbeta_i^T(\bfx_j - \bfx_i) \qquad\forall\ i, j = 1, \dotsc, n.}{cvx-reg-qp}

In particular, we can reduce the infinite-dimensional problem \eqref{cvx-reg} into a finite-dimensional quadratic program (QP) \eqref{cvx-reg-qp}, which can be efficiently solved. The solution to \eqref{cvx-reg-qp} can be viewed as a piecewise-linear convex function. 

Here, we attempt to derive the analogous equivalence, i.e. find an equivalent convex optimization problem to the following infinite-dimensional problem:
\optprob{minimize}{}{\sum_{i=1}^n\inparen{y_i - f(\bfx_i)}^2}{\text{$f$ is an SOS-convex polynomial of degree $2d$.}}{sos-reg}

Denote the vector of basis monomials up to degree $d$ by $\bfv_d(\bfx) = (1, x_1, \dotsc, x_p, x_1x_2, \dotsc, x_p^d)$, where $\bfx = (x_1, \dotsc, x_p)$. Also, let $s = {2d + p \choose p}$ denote the length of this vector for degree $2d$. Then, we may replace $f$ with a coefficient vector $\bftheta \in \R^s$, such that
\[
f(\bfx) = \bftheta^T\bfv_{2d}(\bfx).
\]
Note the one-to-one correspondence between $f$ and $\bftheta$. Further, as done with the convex program, we introduce the auxiliary variable $\bfz = (z_1, \dotsc, z_n)$ so that 
\begin{equation}\label{aux}
f(\bfx_i) = \bftheta^T\bfv_{2d}(\bfx_i) = z_i \qquad\forall\ i = 1, \dotsc, n.
\end{equation}
We can write this more concisely by introducing the matrix
\[
V = V(\bfx_1, \dotsc, \bfx_n) = \begin{bmatrix}
\bfv_d(\bfx_1) \\
\vdots \\
\bfv_d(\bfx_n)
\end{bmatrix}_{n \times s}
\]
so that \eqref{aux} simply becomes
\begin{equation}\label{sos-poly}
V\bftheta = \bfz.
\end{equation}

So we have a linear constraint on the coefficient $\theta$ that is equivalent to saying that the polynomial interpolates the points $\{(\bfx_i, z_i)\}_{i=1}^n$. Analogously, we can rewrite the objective to be
\begin{equation}\label{sos-reg-obj2}
\sum_{i=1}^n(y_i - z_i)^2 = \norm{\bfy - \bfz}^2
\end{equation}
where $\bfy = (y_1, \dotsc, y_n)$ and $\bfz = (z_1, \dotsc, z_n)$.


Now we want to rewrite the constraint that $f(\bfx) = \bftheta^T\bfv_d(\bfx)$ is SOS-convex. Recall that $p$ is SOS-convex if and only if
\begin{equation}\label{sos-cvx}
\bfu^TH_\bftheta(\bfx)\bfu = \bfv_d(\bfx,\bfu)^TQ\bfv_d(\bfx,\bfu)
\end{equation}
and
\begin{equation}\label{sos-cvx-psd}
Q \psd 0
\end{equation}
where $Q$ is a symmetric $r \times r$ matrix with $r = {d + 2p \choose 2p}$, $H_\theta(\bfx)$ is the Hessian polynomial matrix of $p(\bfx) = \bftheta^T\bfv_d(\bfx)$, and $\bfu = (u_1, \dotsc, u_p)$. 

Note first that \eqref{sos-cvx} is an equality in the space of \emph{polynomials} on $2p$ variables: $\bfx$ and $\bfu$. Also, \textbf{\eqref{sos-cvx-psd} suggests that the convex optimization program that we attempt to build is a semidefinite program (SDP).}

It is important to note that, \textbf{in this case where $f$ is a polynomial, the Hessian $H_\theta(\bfx)$ is easy to compute -- in fact, the coefficients of each entry of the Hessian are a linear function of the coefficient $\bftheta$ of the original polynomial $f$.} For a multinomial index $\bfalpha = (\alpha_1, \dotsc, \alpha_p) \in \N^p$ on $\bfx$ such that $\sum_{j=1}^p \alpha_j \leq d$, recall that the Hessian of $\bfx^\bfalpha$ with respect to $x_i$ and $x_j$ is
\[
\frac{\partial}{\partial x_i}\frac{\partial}{\partial x_j} \bfx^\bfalpha = \alpha_i\alpha_j \bfx^{\bfalpha_{i,j}'}
\]
where $\bfalpha_{i,j}' = (\alpha_1, \dotsc, \alpha_i-1, \dotsc, \alpha_j-1, \dotsc, \alpha_p)$ for $i \neq j$ and $\bfalpha_{i,i}' = (\alpha_1, \dotsc, \alpha_i-2, \dotsc, \alpha_p)$. Note that if $\alpha_i = 0$ for any $i$ then the Hessian is zero anyway.

\eqref{sos-cvx} is not a valid semidefinite constraint yet, because it is an equality between two polynomials. This means we want to equate the \emph{coefficients} of the two polynomials on $(\bfx, \bfy)$. Note first that
\begin{equation}\label{hessian}
\bfu^TH_\bftheta(\bfx)\bfu = \sum_{i,j=1}^p \inparen{\frac{\partial}{\partial x_i}\frac{\partial}{\partial x_j}f(\bfx)} u_iu_j.
\end{equation}
Then, there is at most one term (zero iff the partial derivative is zero) for each $\bfalpha = (\alpha_1, \dotsc, \alpha_p) \in \N^p$ such that $\sum_{k=1}^p \alpha_k \leq d$, and for each $i, j = 1, \dotsc, p$. For each of these cases, we give a multinomial index $\bfgamma_{\bfalpha, i, j} \in \N^{2p}$ on $(\bfx, \bfu)$, and $\bfgamma_{\bfalpha, i, j}$ will have a few specific properties: the first $p$ coordinates are precisely $\bfalpha_{i,j}'$, which sum up to at most $d-2$, and the last $p$ coordinates are all zero except at the $(p+i)$th and the $(p+j)$th coordinate (only at the $(p+i)$th if $i = j$). The first corresponds to the fact that the Hessian can have at most degree $d-2$, and the second to the fact that each term has exactly one degree on $u_i$ and $u_j$. Thus, we have
\begin{equation}\label{lhs}
\bfu^TH_\bftheta(\bfx)\bfu = \sum_\bfalpha \sum_{i,j} H_\bftheta(\bfalpha, i, j) (\bfx, \bfu)^{\bfgamma_{\bfalpha, i, j}}
\end{equation}
where $H_\bftheta(\bfalpha, i, j)$ represents the scalar coefficient for the term $(\bfx, \bfu)^{\bfgamma_{\bfalpha, i, j}}$.

Further, we can express the right-hand side in terms of their coordinates in the following way. First define the coordinate matrix $B_\bfgamma$ for each multi-index $\bfgamma \in \N^{2p}$ up to degree $2d$ such that
\[
\bfv_d(\bfx, \bfu)\bfv_d(\bfx, \bfu)^T = \sum_\bfgamma B_\bfgamma (\bfx, \bfu)^\bfgamma.
\]
Note that the matrices $B_\bfgamma$ are simply ``constants'', i.e. they do not depend on the data or the program variables. With this, the right-hand side becomes
\begin{align}\label{rhs}
\bfv_d(\bfx,\bfu)^TQ\bfv_d(\bfx,\bfu) &= \tr(Q\bfv_d(\bfx, \bfu)\bfv_d(\bfx, \bfu)^T) \notag\\
&= \ip{Q, \bfv_d(\bfx, \bfu)\bfv_d(\bfx, \bfu)^T} \notag\\
&= \ip{Q, \sum_\bfgamma B_\bfgamma (\bfx, \bfu)^\bfgamma} \notag\\
&= \sum_\bfgamma \ip{Q, B_\bfgamma}(\bfx, \bfu)^\bfgamma
\end{align}
where $\ip{A,B} = \tr(A^TB) = \sum_{i,j} A_{ij}B_{ij}$ is the matrix inner product. Note that $Q$ is symmetric.

Then, we can equate the coefficients of \eqref{lhs} and \eqref{rhs} to obtain:
\begin{align}
\ip{Q, B_{\bfgamma_{\bfalpha, i, j}}} &= H_\bftheta(\bfalpha, i, j) \qquad \forall\ \bfalpha, i, j \label{sos-cvx-coeff1}\\
\ip{Q, B_{\bfgamma}} &= 0 \qquad\qquad \text{for all other $\bfgamma$} \label{sos-cvx-coeff2}
\end{align}

Putting \eqref{sos-poly}, \eqref{sos-reg-obj2}, \eqref{sos-cvx-psd}, \eqref{sos-cvx-coeff1}, and \eqref{sos-cvx-coeff2} together, \eqref{sos-reg} can be restated as the following problem:

\begin{equation}\label{sos-reg2}
\begin{aligned}
& \underset{\bfz, \bftheta, Q}{\text{minimize}}
& & \norm{\bfy-\bfz}^2 \\
& \text{\quad\ s.t.}
& & V\bftheta = \bfz \\
& & & \langle{Q, B_{\bfgamma_{\bfalpha, i, j}}\rangle} = H_\bftheta(\bfalpha, i, j) \qquad \forall\ \bfalpha, i, j \\
& & & \langle{Q, B_{\bfgamma}\rangle} = 0 \qquad \text{for all other $\bfgamma$} \\
& & & Q \psd 0 
\end{aligned}
\end{equation}

\eqref{sos-reg2} is almost an SDP, except that the objective is quadratic. But in general, we can introduce another auxiliary variable $t$ to restate the problem as
\begin{equation}\label{sos-reg3}
\begin{aligned}
& \underset{t, \bfz, \bftheta, Q}{\text{minimize}}
& & t \\
& \text{\quad\ s.t.}
& & \norm{\bfy-\bfz}^2 \leq t \\
& & & V\bftheta = \bfz \\
& & & \langle{Q, B_{\bfgamma_{\bfalpha, i, j}}\rangle} = H_\bftheta(\bfalpha, i, j) \qquad \forall\ \bfalpha, i, j \\
& & & \langle{Q, B_{\bfgamma}\rangle} = 0 \qquad \text{for all other $\bfgamma$} \\
& & & Q \psd 0 
\end{aligned}
\end{equation}

Then, we are left with a quadratic inequality constraint. Fortunately, the following allows us to convert this into a semidefinite constraint.
\begin{fact}
For any $\bfx, \bfq \in \R^p$ and $r \in \R$, $\bfx^T\bfx + \bfq^T\bfx + r \leq 0$ if and only if $\begin{bmatrix} I & -\bfx \\ -\bfx^T & -\bfq^T\bfx - r \end{bmatrix} \psd 0$.
\end{fact}
\begin{proof}
For any $\bfy \in \R^p$ and $z \in \R$, 
\begin{align*}
\begin{bmatrix} \bfy^T & z \end{bmatrix} \begin{bmatrix} I & -\bfx \\ -\bfx^T & -\bfq^T\bfx - r \end{bmatrix} \begin{bmatrix} \bfy \\ z \end{bmatrix} &= \bfy^T\bfy -2z\bfx^T\bfy - z^2(\bfq^T\bfx+r) \\
&= \norm{\bfy - z\bfx}^2 - z^2(\bfx^T\bfx + \bfq^T\bfx + r).
\end{align*}
If $\bfx^T\bfx + \bfq^T\bfx + r \leq 0$, then this is nonnegative for all $\bfy \in \R^p$ and $z \in \R$. Otherwise, one can find $\bfy \in \R^p$ and $z \in \R$ such that this is strictly negative.
\end{proof}

Thus,
\begin{align*}
\norm{\bfy-\bfz}^2 \leq t &\iff \bfz^T\bfz - 2\bfy^T\bfz + (\bfy^T\bfy - t) \leq 0 \\
&\iff \begin{bmatrix} I & -\bfz \\ -\bfz^T & 2\bfy^T\bfz - \bfy^T\bfy + t \end{bmatrix} \psd 0.
\end{align*}
Note that the last relation is a linear matrix inequality (LMI), i.e. it says that a linear combination of symmetric matrices is positive semidefinite. 

Thus, we can now write \eqref{sos-reg3} into a semidefinite program:
\begin{equation}\label{sos-reg4}
\begin{aligned}
& \underset{t, \bfz, \bftheta, Q}{\text{minimize}}
& & t \\
& \text{\quad\ s.t.}
& & \begin{bmatrix} I & -\bfz \\ -\bfz^T & 2\bfy^T\bfz - \bfy^T\bfy + t \end{bmatrix} \psd 0 \\
& & & V\bftheta = \bfz \\
& & & \langle{Q, B_{\bfgamma_{\bfalpha, i, j}}\rangle} = H_\bftheta(\bfalpha, i, j) \qquad \forall\ \bfalpha, i, j \\
& & & \langle{Q, B_{\bfgamma}\rangle} = 0 \qquad \text{for all other $\bfgamma$} \\
& & & Q \psd 0 
\end{aligned}
\end{equation}
where the two semidefinite constraints can be restated -- if necessary -- into one semidefinite constraint
\[
\begin{bmatrix} I & -\bfz &  \\ -\bfz^T & 2\bfy^T\bfz - \bfy^T\bfy + t & \\ & & Q \end{bmatrix} \psd 0.
\]

Finally, note that the entire program depends on the degree of the SOS-convex polynomial that we started off with: $2d$. 

\subsection*{Further Questions}

\begin{enumerate}
\item What is the program size? Is it tractable?
\item Can the zero constraints be simplified?
\item For any given $d$, is the program feasible? What is the behavior of the objective $t_d$?
\item How can SDP hierarchy (e.g. by Lasserre) help choosing/removing $d$?
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% I have actually re-defined the \cite{} command so that the
% citation will simply display as whatever you name it to be.
%
% There is no bibtex here that you need to run. If you say
% \cite{foo}, the lecture notes will simply display [foo].
%
% You don't need a .bib file. You will need to manually put in
% entries in the format below for whatever references you cite.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section*{References}
\beginrefs
\bibentry{BV}{\sc Boyd, S.} and {\sc Vandenberghe, L.} (2009).
{\it Convex Optimization}.
Cambridge University Press.
\bibentry{Lasserre}{\sc Lasserre, J. B.} (2009).
{\it Moments, Positive Polynomials and Their Applications}. Vol. 1. 
World Scientific.
\endrefs



% **** THIS ENDS THE EXAMPLES. DON'T DELETE THE FOLLOWING LINE:

\end{document}
